{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import time\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Input\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "\n",
    "#from scipy.misc import imread\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop potential randomness\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jbang36/eva/others'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jbang36/eva/data/mnist'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = os.path.abspath('../')\n",
    "data_dir = os.path.join(root_dir, 'data', 'mnist')\n",
    "root_dir\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f42ce30ac88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_index = 7777 # You may select anything up to 60,000\n",
    "print(train_y[image_index]) # The label is 8\n",
    "plt.imshow(train_x[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_jobs=-1, n_clusters=10, n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(-1, 784).astype('float32')\n",
    "test_x = test_x.reshape(-1, 784).astype('float32')\n",
    "train_x /= 255.0\n",
    "test_x /= 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time it took to train k_means is  613.3203248977661  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "km.fit(train_x)\n",
    "print(\"Total time it took to train k_means is \", time.time() - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = km.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "kmeans_score = normalized_mutual_info_score(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kmeans_model.joblib']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(km, 'kmeans_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Failed to interpret file 'kmeans_model.joblib' as a pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda2/envs/pp36/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x00'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-62d92dd91cc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If saved and later resumed...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kmeans_model.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/pp36/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 raise IOError(\n\u001b[0;32m--> 443\u001b[0;31m                     \"Failed to interpret file %s as a pickle\" % repr(file))\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Failed to interpret file 'kmeans_model.joblib' as a pickle"
     ]
    }
   ],
   "source": [
    "# If saved and later resumed...\n",
    "km = load('kmeans_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(500, activation='relu')(input_img)\n",
    "encoded = Dense(500, activation='relu')(encoded)\n",
    "encoded = Dense(2000, activation='relu')(encoded)\n",
    "encoded = Dense(10, activation='sigmoid')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(2000, activation='relu')(encoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(784)(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2000)              22000     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 500)               1000500   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 784)               392784    \n",
      "=================================================================\n",
      "Total params: 3,330,794\n",
      "Trainable params: 3,330,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0741 - val_loss: 0.0643\n",
      "Epoch 2/500\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0638 - val_loss: 0.0635\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0615 - val_loss: 0.0555\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0529 - val_loss: 0.0501\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0474 - val_loss: 0.0428\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0403 - val_loss: 0.0371\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0356 - val_loss: 0.0333\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0321 - val_loss: 0.0304\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0296 - val_loss: 0.0282\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0278 - val_loss: 0.0269\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0263 - val_loss: 0.0253\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0250 - val_loss: 0.0242\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0240 - val_loss: 0.0232\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0230 - val_loss: 0.0225\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0229 - val_loss: 0.0219\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0217 - val_loss: 0.0213\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0213 - val_loss: 0.0211\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0210 - val_loss: 0.0209\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0206 - val_loss: 0.0202\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0201 - val_loss: 0.0200\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0202 - val_loss: 0.0197\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0196 - val_loss: 0.0194\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0194 - val_loss: 0.0192\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0191 - val_loss: 0.0190\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0190 - val_loss: 0.0188\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0186 - val_loss: 0.0185\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0185 - val_loss: 0.0185\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0183 - val_loss: 0.0183\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0181 - val_loss: 0.0181\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0180 - val_loss: 0.0180\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0177 - val_loss: 0.0177\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0177 - val_loss: 0.0177\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0174 - val_loss: 0.0176\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0174 - val_loss: 0.0174\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0172 - val_loss: 0.0173\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0170 - val_loss: 0.0172\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0171 - val_loss: 0.0171\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0165 - val_loss: 0.0167\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0164 - val_loss: 0.0166\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0164 - val_loss: 0.0166\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0162 - val_loss: 0.0166\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0162 - val_loss: 0.0164\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0160 - val_loss: 0.0163\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0160 - val_loss: 0.0163\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0158 - val_loss: 0.0163\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0158 - val_loss: 0.0161\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0157 - val_loss: 0.0163\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0157 - val_loss: 0.0160\n",
      "Epoch 52/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0155 - val_loss: 0.0159\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0155 - val_loss: 0.0161\n",
      "Epoch 54/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0155 - val_loss: 0.0159\n",
      "Epoch 55/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0153 - val_loss: 0.0157\n",
      "Epoch 56/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0153 - val_loss: 0.0158\n",
      "Epoch 57/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0152 - val_loss: 0.0161\n",
      "Epoch 58/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0152 - val_loss: 0.0156\n",
      "Epoch 59/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0151 - val_loss: 0.0157\n",
      "Epoch 60/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0150 - val_loss: 0.0155\n",
      "Epoch 61/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0149 - val_loss: 0.0155\n",
      "Epoch 62/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0149 - val_loss: 0.0154\n",
      "Epoch 63/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0148 - val_loss: 0.0156\n",
      "Epoch 64/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0148 - val_loss: 0.0154\n",
      "Epoch 65/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0147 - val_loss: 0.0153\n",
      "Epoch 66/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0147 - val_loss: 0.0153\n",
      "Epoch 67/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0146 - val_loss: 0.0152\n",
      "Epoch 68/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0145 - val_loss: 0.0152\n",
      "Epoch 69/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0146 - val_loss: 0.0152\n",
      "Epoch 70/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0144 - val_loss: 0.0151\n",
      "Epoch 71/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0145 - val_loss: 0.0153\n",
      "Epoch 72/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0144 - val_loss: 0.0150\n",
      "Epoch 73/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0143 - val_loss: 0.0150\n",
      "Epoch 74/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0143 - val_loss: 0.0150\n",
      "Epoch 75/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0142 - val_loss: 0.0150\n",
      "Epoch 76/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0142 - val_loss: 0.0149\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0142 - val_loss: 0.0149\n",
      "Epoch 78/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0141 - val_loss: 0.0149\n",
      "Epoch 79/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0141 - val_loss: 0.0149\n",
      "Epoch 80/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0140 - val_loss: 0.0148\n",
      "Epoch 81/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0140 - val_loss: 0.0148\n",
      "Epoch 82/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0139 - val_loss: 0.0148\n",
      "Epoch 83/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0139 - val_loss: 0.0148\n",
      "Epoch 84/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0139 - val_loss: 0.0147\n",
      "Epoch 85/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0138 - val_loss: 0.0146\n",
      "Epoch 86/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0138 - val_loss: 0.0148\n",
      "Epoch 87/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0138 - val_loss: 0.0146\n",
      "Epoch 88/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0136 - val_loss: 0.0146\n",
      "Epoch 89/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0137 - val_loss: 0.0146\n",
      "Epoch 90/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0136 - val_loss: 0.0146\n",
      "Epoch 91/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0136 - val_loss: 0.0146\n",
      "Epoch 92/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0135 - val_loss: 0.0145\n",
      "Epoch 93/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0135 - val_loss: 0.0146\n",
      "Epoch 94/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0135 - val_loss: 0.0146\n",
      "Epoch 95/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0135 - val_loss: 0.0146\n",
      "Epoch 96/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0134 - val_loss: 0.0143\n",
      "Epoch 97/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0134 - val_loss: 0.0143\n",
      "Epoch 98/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0134 - val_loss: 0.0143\n",
      "Epoch 99/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0133 - val_loss: 0.0144\n",
      "Epoch 100/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0132 - val_loss: 0.0143\n",
      "Epoch 101/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0133 - val_loss: 0.0143\n",
      "Epoch 102/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0132 - val_loss: 0.0143\n",
      "Epoch 103/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0132 - val_loss: 0.0143\n",
      "Epoch 104/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0132 - val_loss: 0.0142\n",
      "Epoch 105/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0131 - val_loss: 0.0141\n",
      "Epoch 106/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0131 - val_loss: 0.0144\n",
      "Epoch 107/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0132 - val_loss: 0.0141\n",
      "Epoch 108/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0130 - val_loss: 0.0142\n",
      "Epoch 109/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0130 - val_loss: 0.0141\n",
      "Epoch 110/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0130 - val_loss: 0.0140\n",
      "Epoch 111/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0130 - val_loss: 0.0140\n",
      "Epoch 112/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0129 - val_loss: 0.0141\n",
      "Epoch 113/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0129 - val_loss: 0.0140\n",
      "Epoch 114/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0129 - val_loss: 0.0141\n",
      "Epoch 115/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0128 - val_loss: 0.0141\n",
      "Epoch 116/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0128 - val_loss: 0.0140\n",
      "Epoch 117/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0128 - val_loss: 0.0139\n",
      "Epoch 118/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0128 - val_loss: 0.0140\n",
      "Epoch 119/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0127 - val_loss: 0.0140\n",
      "Epoch 120/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0127 - val_loss: 0.0139\n",
      "Epoch 121/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0127 - val_loss: 0.0139\n",
      "Epoch 122/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0127 - val_loss: 0.0141\n",
      "Epoch 123/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0126 - val_loss: 0.0139\n",
      "Epoch 124/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0127 - val_loss: 0.0138\n",
      "Epoch 125/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0125 - val_loss: 0.0139\n",
      "Epoch 126/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0126 - val_loss: 0.0140\n",
      "Epoch 127/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0125 - val_loss: 0.0138\n",
      "Epoch 128/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0125 - val_loss: 0.0138\n",
      "Epoch 129/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0125 - val_loss: 0.0138\n",
      "Epoch 130/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0125 - val_loss: 0.0139\n",
      "Epoch 131/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0124 - val_loss: 0.0138\n",
      "Epoch 132/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0124 - val_loss: 0.0137\n",
      "Epoch 133/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0124 - val_loss: 0.0137\n",
      "Epoch 134/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0124 - val_loss: 0.0137\n",
      "Epoch 135/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0124 - val_loss: 0.0137\n",
      "Epoch 136/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0124 - val_loss: 0.0137\n",
      "Epoch 137/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0124 - val_loss: 0.0137\n",
      "Epoch 138/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0123 - val_loss: 0.0138\n",
      "Epoch 139/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0123 - val_loss: 0.0137\n",
      "Epoch 140/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0123 - val_loss: 0.0136\n",
      "Epoch 141/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0122 - val_loss: 0.0137\n",
      "Epoch 142/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0122 - val_loss: 0.0136\n",
      "Epoch 143/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0122 - val_loss: 0.0136\n",
      "Epoch 144/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0122 - val_loss: 0.0136\n",
      "Epoch 145/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0122 - val_loss: 0.0137\n",
      "Epoch 146/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0121 - val_loss: 0.0135\n",
      "Epoch 147/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0121 - val_loss: 0.0135\n",
      "Epoch 148/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0121 - val_loss: 0.0135\n",
      "Epoch 149/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0121 - val_loss: 0.0135\n",
      "Epoch 150/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0121 - val_loss: 0.0136\n",
      "Epoch 151/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0120 - val_loss: 0.0135\n",
      "Epoch 152/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0120 - val_loss: 0.0134\n",
      "Epoch 153/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0120 - val_loss: 0.0136\n",
      "Epoch 154/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0120 - val_loss: 0.0136\n",
      "Epoch 155/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0120 - val_loss: 0.0135\n",
      "Epoch 156/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0120 - val_loss: 0.0134\n",
      "Epoch 157/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0119 - val_loss: 0.0135\n",
      "Epoch 158/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0119 - val_loss: 0.0135\n",
      "Epoch 159/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0120 - val_loss: 0.0134\n",
      "Epoch 160/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0119 - val_loss: 0.0134\n",
      "Epoch 161/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0118 - val_loss: 0.0134\n",
      "Epoch 162/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0119 - val_loss: 0.0134\n",
      "Epoch 163/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0118 - val_loss: 0.0134\n",
      "Epoch 164/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0119 - val_loss: 0.0135\n",
      "Epoch 165/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0118 - val_loss: 0.0134\n",
      "Epoch 166/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0118 - val_loss: 0.0133\n",
      "Epoch 167/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0117 - val_loss: 0.0134\n",
      "Epoch 168/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0117 - val_loss: 0.0133\n",
      "Epoch 169/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0118 - val_loss: 0.0134\n",
      "Epoch 170/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0118 - val_loss: 0.0134\n",
      "Epoch 171/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0117 - val_loss: 0.0134\n",
      "Epoch 172/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0117 - val_loss: 0.0133\n",
      "Epoch 173/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0117 - val_loss: 0.0133\n",
      "Epoch 174/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0116 - val_loss: 0.0133\n",
      "Epoch 175/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0116 - val_loss: 0.0133\n",
      "Epoch 176/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0117 - val_loss: 0.0132\n",
      "Epoch 177/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0116 - val_loss: 0.0133\n",
      "Epoch 178/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0116 - val_loss: 0.0133\n",
      "Epoch 179/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0116 - val_loss: 0.0133\n",
      "Epoch 180/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0116 - val_loss: 0.0133\n",
      "Epoch 181/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0116 - val_loss: 0.0133\n",
      "Epoch 182/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0116 - val_loss: 0.0133\n",
      "Epoch 183/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0116 - val_loss: 0.0132\n",
      "Epoch 184/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0115 - val_loss: 0.0132\n",
      "Epoch 185/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0115 - val_loss: 0.0132\n",
      "Epoch 186/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0115 - val_loss: 0.0133\n",
      "Epoch 187/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0115 - val_loss: 0.0133\n",
      "Epoch 188/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0115 - val_loss: 0.0132\n",
      "Epoch 189/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0114 - val_loss: 0.0132\n",
      "Epoch 190/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0115 - val_loss: 0.0132\n",
      "Epoch 191/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0115 - val_loss: 0.0132\n",
      "Epoch 192/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0114 - val_loss: 0.0133\n",
      "Epoch 193/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0114 - val_loss: 0.0132\n",
      "Epoch 194/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0115 - val_loss: 0.0132\n",
      "Epoch 195/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0114 - val_loss: 0.0131\n",
      "Epoch 196/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0114 - val_loss: 0.0132\n",
      "Epoch 197/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0114 - val_loss: 0.0132\n",
      "Epoch 198/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0114 - val_loss: 0.0132\n",
      "Epoch 199/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0114 - val_loss: 0.0131\n",
      "Epoch 200/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0113 - val_loss: 0.0132\n",
      "Epoch 201/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0113 - val_loss: 0.0131\n",
      "Epoch 202/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0114 - val_loss: 0.0131\n",
      "Epoch 203/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0113 - val_loss: 0.0131\n",
      "Epoch 204/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0113 - val_loss: 0.0133\n",
      "Epoch 205/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0113 - val_loss: 0.0132\n",
      "Epoch 206/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0113 - val_loss: 0.0132\n",
      "Epoch 207/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0113 - val_loss: 0.0131\n",
      "Epoch 208/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0112 - val_loss: 0.0131\n",
      "Epoch 209/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0112 - val_loss: 0.0133\n",
      "Epoch 210/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0112 - val_loss: 0.0131\n",
      "Epoch 211/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0112 - val_loss: 0.0131\n",
      "Epoch 212/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0113 - val_loss: 0.0132\n",
      "Epoch 213/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0112 - val_loss: 0.0131\n",
      "Epoch 214/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0111 - val_loss: 0.0130\n",
      "Epoch 215/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0112 - val_loss: 0.0132\n",
      "Epoch 216/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0112 - val_loss: 0.0131\n",
      "Epoch 217/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0112 - val_loss: 0.0131\n",
      "Epoch 218/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0111 - val_loss: 0.0130\n",
      "Epoch 219/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0111 - val_loss: 0.0131\n",
      "Epoch 220/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0111 - val_loss: 0.0130\n",
      "Epoch 221/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0112 - val_loss: 0.0132\n",
      "Epoch 222/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0111 - val_loss: 0.0130\n",
      "Epoch 223/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0110 - val_loss: 0.0130\n",
      "Epoch 224/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0111 - val_loss: 0.0131\n",
      "Epoch 225/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0111 - val_loss: 0.0130\n",
      "Epoch 226/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0111 - val_loss: 0.0130\n",
      "Epoch 227/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0110 - val_loss: 0.0130\n",
      "Epoch 228/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0110 - val_loss: 0.0131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0111 - val_loss: 0.0132\n",
      "Epoch 230/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0110 - val_loss: 0.0130\n",
      "Epoch 231/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0110 - val_loss: 0.0132\n",
      "Epoch 232/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0110 - val_loss: 0.0129\n",
      "Epoch 233/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0130\n",
      "Epoch 234/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0111 - val_loss: 0.0130\n",
      "Epoch 235/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0130\n",
      "Epoch 236/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0130\n",
      "Epoch 237/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0110 - val_loss: 0.0130\n",
      "Epoch 238/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0130\n",
      "Epoch 239/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0130\n",
      "Epoch 240/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0130\n",
      "Epoch 241/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0129\n",
      "Epoch 242/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0131\n",
      "Epoch 243/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0129\n",
      "Epoch 244/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0130\n",
      "Epoch 245/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0130\n",
      "Epoch 246/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0109 - val_loss: 0.0129\n",
      "Epoch 247/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0108 - val_loss: 0.0133\n",
      "Epoch 248/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0129\n",
      "Epoch 249/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109 - val_loss: 0.0131\n",
      "Epoch 250/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0108 - val_loss: 0.0129\n",
      "Epoch 251/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0108 - val_loss: 0.0131\n",
      "Epoch 252/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0108 - val_loss: 0.0129\n",
      "Epoch 253/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0108 - val_loss: 0.0129\n",
      "Epoch 254/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0108 - val_loss: 0.0130\n",
      "Epoch 255/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0108 - val_loss: 0.0130\n",
      "Epoch 256/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0108 - val_loss: 0.0129\n",
      "Epoch 257/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0107 - val_loss: 0.0130\n",
      "Epoch 258/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0108 - val_loss: 0.0129\n",
      "Epoch 259/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0108 - val_loss: 0.0129\n",
      "Epoch 260/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0107 - val_loss: 0.0129\n",
      "Epoch 261/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0107 - val_loss: 0.0130\n",
      "Epoch 262/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0108 - val_loss: 0.0129\n",
      "Epoch 263/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0107 - val_loss: 0.0129\n",
      "Epoch 264/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0108 - val_loss: 0.0129\n",
      "Epoch 265/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0108 - val_loss: 0.0128\n",
      "Epoch 266/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0107 - val_loss: 0.0129\n",
      "Epoch 267/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0107 - val_loss: 0.0128\n",
      "Epoch 268/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0107 - val_loss: 0.0130\n",
      "Epoch 269/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0107 - val_loss: 0.0129\n",
      "Epoch 270/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0106 - val_loss: 0.0129\n",
      "Epoch 271/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0107 - val_loss: 0.0130\n",
      "Epoch 272/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0107 - val_loss: 0.0128\n",
      "Epoch 273/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0106 - val_loss: 0.0129\n",
      "Epoch 274/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0107 - val_loss: 0.0128\n",
      "Epoch 275/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0107 - val_loss: 0.0129\n",
      "Epoch 276/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0106 - val_loss: 0.0128\n",
      "Epoch 277/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0106 - val_loss: 0.0128\n",
      "Epoch 278/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0106 - val_loss: 0.0128\n",
      "Epoch 279/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0106 - val_loss: 0.0129\n",
      "Epoch 280/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0106 - val_loss: 0.0129\n",
      "Epoch 281/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0106 - val_loss: 0.0130\n",
      "Epoch 282/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0106 - val_loss: 0.0128\n",
      "Epoch 283/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0106 - val_loss: 0.0128\n",
      "Epoch 284/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0105 - val_loss: 0.0129\n",
      "Epoch 285/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0106 - val_loss: 0.0128\n",
      "Epoch 286/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0106 - val_loss: 0.0129\n",
      "Epoch 287/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 288/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0106 - val_loss: 0.0129\n",
      "Epoch 289/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 290/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 291/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 292/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0105 - val_loss: 0.0129\n",
      "Epoch 293/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0106 - val_loss: 0.0128\n",
      "Epoch 294/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 295/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0105 - val_loss: 0.0127\n",
      "Epoch 296/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 297/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 298/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 299/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0105 - val_loss: 0.0129\n",
      "Epoch 300/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 301/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 302/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0104 - val_loss: 0.0128\n",
      "Epoch 303/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0104 - val_loss: 0.0128\n",
      "Epoch 304/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0104 - val_loss: 0.0128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0105 - val_loss: 0.0127\n",
      "Epoch 306/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0104 - val_loss: 0.0128\n",
      "Epoch 307/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0104 - val_loss: 0.0128\n",
      "Epoch 308/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0105 - val_loss: 0.0129\n",
      "Epoch 309/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0104 - val_loss: 0.0128\n",
      "Epoch 310/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 311/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0104 - val_loss: 0.0128\n",
      "Epoch 312/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 313/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 314/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 315/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0104 - val_loss: 0.0128\n",
      "Epoch 316/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 317/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0103 - val_loss: 0.0128\n",
      "Epoch 318/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 319/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0104 - val_loss: 0.0128\n",
      "Epoch 320/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 321/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 322/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 323/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 324/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0129\n",
      "Epoch 325/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 326/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 327/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0128\n",
      "Epoch 328/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 329/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0128\n",
      "Epoch 330/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 331/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 332/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 333/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 334/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0102 - val_loss: 0.0128\n",
      "Epoch 335/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0128\n",
      "Epoch 336/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 337/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 338/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 339/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 340/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 341/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 342/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 343/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 344/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 345/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 346/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 347/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 348/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0102 - val_loss: 0.0128\n",
      "Epoch 349/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 350/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 351/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 352/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 353/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 354/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 355/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 356/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 357/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 358/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0101 - val_loss: 0.0128\n",
      "Epoch 359/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 360/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 361/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 362/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 363/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 364/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 365/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 366/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 367/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 368/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 369/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 370/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 371/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 372/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 373/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 374/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 375/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 376/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 377/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 378/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 379/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 380/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0100 - val_loss: 0.0127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 382/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 383/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 384/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 385/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 386/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 387/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 388/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0101 - val_loss: 0.0128\n",
      "Epoch 389/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 390/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 391/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 392/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0127\n",
      "Epoch 393/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 394/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 395/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 396/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 397/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0127\n",
      "Epoch 398/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 399/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0127\n",
      "Epoch 400/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0127\n",
      "Epoch 401/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 402/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0127\n",
      "Epoch 403/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 404/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0127\n",
      "Epoch 405/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 406/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 407/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 408/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 409/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 410/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 411/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0129\n",
      "Epoch 412/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 413/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 414/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 415/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 416/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0127\n",
      "Epoch 417/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0098 - val_loss: 0.0125\n",
      "Epoch 418/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 419/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0125\n",
      "Epoch 420/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 421/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0127\n",
      "Epoch 422/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 423/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 424/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 425/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 426/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0099 - val_loss: 0.0125\n",
      "Epoch 427/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0098 - val_loss: 0.0125\n",
      "Epoch 428/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 429/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 430/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 431/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 432/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0127\n",
      "Epoch 433/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0125\n",
      "Epoch 434/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 435/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 436/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 437/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 438/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 439/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 440/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 441/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0098 - val_loss: 0.0127\n",
      "Epoch 442/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 443/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 444/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0127\n",
      "Epoch 445/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0127\n",
      "Epoch 446/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 447/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 448/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 449/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 450/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 451/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 452/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 453/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 454/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 455/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 456/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - val_loss: 0.0126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 458/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - val_loss: 0.0127\n",
      "Epoch 459/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 460/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 461/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 462/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 463/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 464/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0127\n",
      "Epoch 465/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0127\n",
      "Epoch 466/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 467/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 468/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 469/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 470/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 471/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0125\n",
      "Epoch 472/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 473/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 474/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 475/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 476/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 477/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 478/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0097 - val_loss: 0.0126\n",
      "Epoch 479/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 480/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 481/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 482/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 483/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 484/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 485/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 486/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0127\n",
      "Epoch 487/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 488/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 489/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 490/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 491/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 492/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 493/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 494/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 495/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0126\n",
      "Epoch 496/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 497/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0095 - val_loss: 0.0126\n",
      "Epoch 498/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 499/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0095 - val_loss: 0.0125\n",
      "Epoch 500/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0095 - val_loss: 0.0126\n",
      "Total time it took to train autoencoder is  839.84423661232  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_history = autoencoder.fit(train_x, train_x, epochs=500, batch_size=2048, validation_data=(test_x, test_x))\n",
    "print(\"Total time it took to train autoencoder is \", time.time() - start_time, \" seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_auto_train = encoder.predict(train_x)\n",
    "pred_auto = encoder.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30638918, 0.3083077 , 0.2891475 , 0.5397385 , 0.5570198 ,\n",
       "        0.1033294 , 0.7889402 , 0.19059677, 0.14041516, 0.5033428 ],\n",
       "       [0.6333304 , 0.5814212 , 0.3109706 , 0.7593138 , 0.1580708 ,\n",
       "        0.59077203, 0.6797619 , 0.5381094 , 0.45746633, 0.3270284 ],\n",
       "       [0.00685574, 0.5033929 , 0.545037  , 0.38703018, 0.28323677,\n",
       "        0.4048548 , 0.0600888 , 0.4210693 , 0.65908587, 0.6842213 ],\n",
       "       [0.38829425, 0.07377792, 0.37054238, 0.07115202, 0.7810839 ,\n",
       "        0.4915377 , 0.5093122 , 0.1222934 , 0.33256793, 0.301408  ],\n",
       "       [0.5668864 , 0.36724213, 0.8435695 , 0.4090237 , 0.500612  ,\n",
       "        0.55550164, 0.20108221, 0.18654056, 0.5085922 , 0.7855764 ],\n",
       "       [0.5826199 , 0.31548688, 0.19541757, 0.30895412, 0.5365195 ,\n",
       "        0.41809508, 0.59188205, 0.4620065 , 0.8291928 , 0.43834618],\n",
       "       [0.5384773 , 0.44078165, 0.8627008 , 0.31849614, 0.7064348 ,\n",
       "        0.28452218, 0.48406264, 0.34871268, 0.17418975, 0.32987618],\n",
       "       [0.42272913, 0.55940664, 0.107153  , 0.23237132, 0.5541981 ,\n",
       "        0.41787198, 0.7680428 , 0.45501858, 0.42969415, 0.7850358 ],\n",
       "       [0.5344415 , 0.28442648, 0.8487408 , 0.19131713, 0.51747775,\n",
       "        0.22038554, 0.4272264 , 0.14750494, 0.07794686, 0.24431491],\n",
       "       [0.36134496, 0.17576168, 0.813911  , 0.17601283, 0.26083282,\n",
       "        0.85350144, 0.46196082, 0.21078587, 0.8088622 , 0.3837109 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_auto_train[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_auto = KMeans(n_jobs=-1, n_clusters=10, n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time it took to train k_means is  9.660696744918823  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "km_auto.fit(pred_auto_train)\n",
    "print(\"Total time it took to train k_means is \", time.time() - start_time, \" seconds\")\n",
    "\n",
    "pred = km_auto.predict(pred_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8045266379126781"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_score = normalized_mutual_info_score(test_y, pred)\n",
    "auto_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kmeans_model_autoencoder.joblib']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(km_auto, 'kmeans_model_autoencoder.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If saved and later resumed...\n",
    "km_auto = load('kmeans_model_autoencoder.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0861\n",
      "        |==>  acc: 0.0935,  nmi: 0.0989  <==|\n",
      "Epoch 2/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0642\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0635\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0633\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0625\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0567\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0534\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0482\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0415\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0355\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0315\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0293\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0278\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0260\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0245\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0233\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0221\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0215\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0205\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0199\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0194\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0189\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0184\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0182\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0178\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0175\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0173\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0171\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0169\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0166\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0165\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0163\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0161\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0160\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0159\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0157\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0157\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0154\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0154\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0152\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0151\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0150\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0150\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0149\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0147\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0146\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0146\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0145\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0144\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0143\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0143\n",
      "        |==>  acc: 0.0196,  nmi: 0.6981  <==|\n",
      "Epoch 52/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0144"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0142\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0141\n",
      "Epoch 54/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0140\n",
      "Epoch 55/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0139\n",
      "Epoch 56/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0139\n",
      "Epoch 57/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0138\n",
      "Epoch 58/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0138\n",
      "Epoch 59/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0137\n",
      "Epoch 60/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0137\n",
      "Epoch 61/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0135\n",
      "Epoch 62/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0136\n",
      "Epoch 63/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0134\n",
      "Epoch 64/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0134\n",
      "Epoch 65/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0134\n",
      "Epoch 66/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0133\n",
      "Epoch 67/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0132\n",
      "Epoch 68/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0132\n",
      "Epoch 69/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0132\n",
      "Epoch 70/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0131\n",
      "Epoch 71/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0130\n",
      "Epoch 72/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0130\n",
      "Epoch 73/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0129\n",
      "Epoch 74/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0129\n",
      "Epoch 75/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0129\n",
      "Epoch 76/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0128\n",
      "Epoch 77/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0128\n",
      "Epoch 78/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0127\n",
      "Epoch 79/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0127\n",
      "Epoch 80/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0127\n",
      "Epoch 81/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0126\n",
      "Epoch 82/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0125\n",
      "Epoch 83/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0126\n",
      "Epoch 84/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0125\n",
      "Epoch 85/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0124\n",
      "Epoch 86/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0125\n",
      "Epoch 87/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0123\n",
      "Epoch 88/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0123\n",
      "Epoch 89/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0123\n",
      "Epoch 90/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0123\n",
      "Epoch 91/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0122\n",
      "Epoch 92/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0122\n",
      "Epoch 93/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0122\n",
      "Epoch 94/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0121\n",
      "Epoch 95/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0121\n",
      "Epoch 96/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0121\n",
      "Epoch 97/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0120\n",
      "Epoch 98/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0120\n",
      "Epoch 99/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0120\n",
      "Epoch 100/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0120\n",
      "Epoch 101/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0120\n",
      "        |==>  acc: 0.1176,  nmi: 0.7413  <==|\n",
      "Epoch 102/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0118\n",
      "Epoch 103/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0119\n",
      "Epoch 104/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0118\n",
      "Epoch 105/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0119\n",
      "Epoch 106/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0118\n",
      "Epoch 107/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0117\n",
      "Epoch 108/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0117\n",
      "Epoch 109/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0117\n",
      "Epoch 110/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0116\n",
      "Epoch 111/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0116\n",
      "Epoch 112/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0116\n",
      "Epoch 113/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0116\n",
      "Epoch 114/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0115\n",
      "Epoch 115/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0115\n",
      "Epoch 116/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0115\n",
      "Epoch 117/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0114\n",
      "Epoch 118/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0114\n",
      "Epoch 119/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0115\n",
      "Epoch 120/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0114\n",
      "Epoch 121/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113\n",
      "Epoch 122/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113\n",
      "Epoch 123/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0114\n",
      "Epoch 124/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113\n",
      "Epoch 125/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0112\n",
      "Epoch 126/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113\n",
      "Epoch 127/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0113\n",
      "Epoch 128/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0112\n",
      "Epoch 129/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0111\n",
      "Epoch 130/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0112\n",
      "Epoch 131/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0111\n",
      "Epoch 132/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0112\n",
      "Epoch 133/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0111\n",
      "Epoch 134/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0111\n",
      "Epoch 135/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0110\n",
      "Epoch 136/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0110\n",
      "Epoch 137/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0110\n",
      "Epoch 138/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0110\n",
      "Epoch 139/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0110\n",
      "Epoch 140/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0109\n",
      "Epoch 141/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0110\n",
      "Epoch 142/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0109\n",
      "Epoch 143/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0109\n",
      "Epoch 144/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0109\n",
      "Epoch 145/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0108\n",
      "Epoch 146/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0108\n",
      "Epoch 147/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0109\n",
      "Epoch 148/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0108\n",
      "Epoch 149/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0108\n",
      "Epoch 150/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0108\n",
      "Epoch 151/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107\n",
      "        |==>  acc: 0.0411,  nmi: 0.7550  <==|\n",
      "Epoch 152/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0106"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0107\n",
      "Epoch 153/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0108\n",
      "Epoch 154/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107\n",
      "Epoch 155/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0107\n",
      "Epoch 156/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107\n",
      "Epoch 157/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0107\n",
      "Epoch 158/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106\n",
      "Epoch 159/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0107\n",
      "Epoch 160/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106\n",
      "Epoch 161/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0106\n",
      "Epoch 162/500\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0106\n",
      "Epoch 163/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106\n",
      "Epoch 164/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0105\n",
      "Epoch 165/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0105\n",
      "Epoch 166/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0106\n",
      "Epoch 167/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0105\n",
      "Epoch 168/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0104\n",
      "Epoch 169/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0105\n",
      "Epoch 170/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0105\n",
      "Epoch 171/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0105\n",
      "Epoch 172/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0105\n",
      "Epoch 173/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0104\n",
      "Epoch 174/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103\n",
      "Epoch 175/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104\n",
      "Epoch 176/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0104\n",
      "Epoch 177/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103\n",
      "Epoch 178/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0103\n",
      "Epoch 179/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0103\n",
      "Epoch 180/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0103\n",
      "Epoch 181/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103\n",
      "Epoch 182/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103\n",
      "Epoch 183/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0102\n",
      "Epoch 184/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 185/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 186/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 187/500\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0103\n",
      "Epoch 188/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0102\n",
      "Epoch 189/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0102\n",
      "Epoch 190/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 191/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 192/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102\n",
      "Epoch 193/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0102\n",
      "Epoch 194/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0101\n",
      "Epoch 195/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101\n",
      "Epoch 196/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0100\n",
      "Epoch 197/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101\n",
      "Epoch 198/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0101\n",
      "Epoch 199/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101\n",
      "Epoch 200/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101\n",
      "Epoch 201/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0101\n",
      "        |==>  acc: 0.0119,  nmi: 0.7606  <==|\n",
      "Epoch 202/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0099"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0100\n",
      "Epoch 203/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0100\n",
      "Epoch 204/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0100\n",
      "Epoch 205/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101\n",
      "Epoch 206/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0100\n",
      "Epoch 207/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0100\n",
      "Epoch 208/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0100\n",
      "Epoch 209/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0099\n",
      "Epoch 210/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0100\n",
      "Epoch 211/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0099\n",
      "Epoch 212/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0099\n",
      "Epoch 213/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0099\n",
      "Epoch 214/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0099\n",
      "Epoch 215/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0098\n",
      "Epoch 216/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0099\n",
      "Epoch 217/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0099\n",
      "Epoch 218/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0098\n",
      "Epoch 219/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0099\n",
      "Epoch 220/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0098\n",
      "Epoch 221/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0098\n",
      "Epoch 222/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0098\n",
      "Epoch 223/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0098\n",
      "Epoch 224/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0098\n",
      "Epoch 225/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0099\n",
      "Epoch 226/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 227/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0097\n",
      "Epoch 228/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0098\n",
      "Epoch 229/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0097\n",
      "Epoch 230/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0097\n",
      "Epoch 231/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 232/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 233/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0098\n",
      "Epoch 234/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 235/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0096\n",
      "Epoch 236/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 237/500\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0097\n",
      "Epoch 238/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0096\n",
      "Epoch 239/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 240/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 241/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0097\n",
      "Epoch 242/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0096\n",
      "Epoch 243/500\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0096\n",
      "Epoch 244/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0096\n",
      "Epoch 245/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0096\n",
      "Epoch 246/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0096\n",
      "Epoch 247/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0096\n",
      "Epoch 248/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0095\n",
      "Epoch 249/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0095\n",
      "Epoch 250/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0096\n",
      "Epoch 251/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0096\n",
      "        |==>  acc: 0.1732,  nmi: 0.7648  <==|\n",
      "Epoch 252/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0095\n",
      "Epoch 253/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 254/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 255/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0096\n",
      "Epoch 256/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 257/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0095\n",
      "Epoch 258/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 259/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 260/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 261/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 262/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 263/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 264/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 265/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 266/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 267/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 268/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0094\n",
      "Epoch 269/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0095\n",
      "Epoch 270/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 271/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 272/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 273/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 274/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 275/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 276/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 277/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 278/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0094\n",
      "Epoch 279/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0094\n",
      "Epoch 280/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0093\n",
      "Epoch 281/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0093\n",
      "Epoch 282/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 283/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 284/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 285/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 286/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 287/500\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0093\n",
      "Epoch 288/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0093\n",
      "Epoch 289/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 290/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0092\n",
      "Epoch 291/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 292/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0092\n",
      "Epoch 293/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0092\n",
      "Epoch 294/500\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0093\n",
      "Epoch 295/500\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0092\n",
      "Epoch 296/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0093\n",
      "Epoch 297/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 298/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0093\n",
      "Epoch 299/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 300/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 301/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "        |==>  acc: 0.0069,  nmi: 0.7724  <==|\n",
      "Epoch 302/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0093"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0092\n",
      "Epoch 303/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 304/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 305/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 306/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0092\n",
      "Epoch 307/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 308/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0091\n",
      "Epoch 309/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0092\n",
      "Epoch 310/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 311/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 312/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 313/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 314/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 315/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 316/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 317/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0092\n",
      "Epoch 318/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 319/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 320/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 321/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 322/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0091\n",
      "Epoch 323/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 324/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 325/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 326/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0091\n",
      "Epoch 327/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 328/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 329/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 330/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 331/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 332/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 333/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 334/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 335/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 336/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 337/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 338/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 339/500\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0089\n",
      "Epoch 340/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 341/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 342/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 343/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 344/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0090\n",
      "Epoch 345/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0090\n",
      "Epoch 346/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 347/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 348/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 349/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 350/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 351/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "        |==>  acc: 0.0985,  nmi: 0.7671  <==|\n",
      "Epoch 352/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0090"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0089\n",
      "Epoch 353/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 354/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 355/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0089\n",
      "Epoch 356/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 357/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 358/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0089\n",
      "Epoch 359/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 360/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 361/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 362/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 363/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 364/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 365/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 366/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 367/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 368/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 369/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 370/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 371/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 372/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 373/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 374/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 375/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 376/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 377/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0088\n",
      "Epoch 378/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 379/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 380/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 381/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 382/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 383/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 384/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 385/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0088\n",
      "Epoch 386/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 387/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 388/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 389/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 390/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 391/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 392/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 393/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 394/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 395/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 396/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 397/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 398/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 399/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 400/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 401/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "        |==>  acc: 0.1019,  nmi: 0.7714  <==|\n",
      "Epoch 402/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0086\n",
      "Epoch 403/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 404/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 405/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 406/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 407/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 408/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 409/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 410/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 411/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0086\n",
      "Epoch 412/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0087\n",
      "Epoch 413/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0086\n",
      "Epoch 414/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0087\n",
      "Epoch 415/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 416/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 417/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 418/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 419/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 420/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 421/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 422/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 423/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 424/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 425/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 426/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 427/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0086\n",
      "Epoch 428/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 429/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 430/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 431/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 432/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 433/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 434/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 435/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0086\n",
      "Epoch 436/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 437/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 438/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 439/500\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0084\n",
      "Epoch 440/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 441/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 442/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 443/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 444/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 445/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 446/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 447/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 448/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 449/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 450/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 451/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "        |==>  acc: 0.0612,  nmi: 0.7731  <==|\n",
      "Epoch 452/500\n",
      " 4096/60000 [=>............................] - ETA: 1s - loss: 0.0086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0085\n",
      "Epoch 453/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 454/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0084\n",
      "Epoch 455/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0085\n",
      "Epoch 456/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0085\n",
      "Epoch 457/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 458/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 459/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 460/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 461/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 462/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 463/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 464/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 465/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 466/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 467/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 468/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 469/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 470/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 471/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 472/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 473/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 474/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 475/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 476/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 477/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 478/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 479/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 480/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Epoch 481/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 482/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Epoch 483/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 484/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0084\n",
      "Epoch 485/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 486/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 487/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 488/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084\n",
      "Epoch 489/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 490/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 491/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 492/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Epoch 493/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0083\n",
      "Epoch 494/500\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0083\n",
      "Epoch 495/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Epoch 496/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 497/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0083\n",
      "Epoch 498/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0082\n",
      "Epoch 499/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Epoch 500/500\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0083\n",
      "Pretraining time:  877.1099791526794\n",
      "Pretrained weights are saved to results/ae_weights.h5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Keras implementation for Deep Embedded Clustering (DEC) algorithm:\n",
    "\n",
    "Original Author:\n",
    "    Xifeng Guo. 2017.1.30\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')\n",
    "\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DEC(object):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0,\n",
    "                 init='glorot_uniform'):\n",
    "\n",
    "        super(DEC, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.autoencoder, self.encoder = autoencoder(self.dims, init=init)\n",
    "\n",
    "        # prepare DEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
    "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
    "\n",
    "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
    "        print('...Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "        cb = [csv_logger]\n",
    "        if y is not None:\n",
    "            class PrintACC(callbacks.Callback):\n",
    "                def __init__(self, x, y):\n",
    "                    self.x = x\n",
    "                    self.y = y\n",
    "                    super(PrintACC, self).__init__()\n",
    "\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    if epoch % int(epochs/10) != 0:\n",
    "                        return\n",
    "                    feature_model = Model(self.model.input,\n",
    "                                          self.model.get_layer(\n",
    "                                              'encoder_%d' % (int(len(self.model.layers) / 2) - 1)).output)\n",
    "                    features = feature_model.predict(self.x)\n",
    "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20, n_jobs=4)\n",
    "                    y_pred = km.fit_predict(features)\n",
    "                    # print()\n",
    "                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (metrics.accuracy_score(self.y, y_pred), metrics.normalized_mutual_info_score(self.y, y_pred)))\n",
    "\n",
    "            cb.append(PrintACC(x, y))\n",
    "\n",
    "        # begin pretraining\n",
    "        t0 = time.time()\n",
    "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb)\n",
    "        print('Pretraining time: ', time.time() - t0)\n",
    "        self.autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "        print('Pretrained weights are saved to %s/ae_weights.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of DEC model\n",
    "        self.model.load_weights(weights)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, optimizer='sgd', loss='kld'):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n",
    "            update_interval=140, save_dir='./results/temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = x.shape[0] / batch_size * 5  # 5 epochs\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time.time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        # logging file\n",
    "        import csv\n",
    "        logfile = open(save_dir + '/dec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        index_array = np.arange(x.shape[0])\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(metrics.accuracy_score(y, y_pred), 5)\n",
    "                    nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n",
    "                    ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    \n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = np.copy(y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            # if index == 0:\n",
    "            #     np.random.shuffle(index_array)\n",
    "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "            self.model.train_on_batch(x=x[idx], y=p[idx])\n",
    "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# setting the hyper parameters\n",
    "init = 'glorot_uniform'\n",
    "pretrain_optimizer = 'adam'\n",
    "dataset = 'mnist'\n",
    "batch_size = 2048\n",
    "maxiter = 2e4\n",
    "tol = 0.0001\n",
    "save_dir = 'results'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "update_interval = 200\n",
    "pretrain_epochs = 500\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                       distribution='uniform')  # [-limit, limit], limit=sqrt(1./fan_in)\n",
    "#pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "\n",
    "\n",
    "# prepare the DEC model\n",
    "dec = DEC(dims=[train_x.shape[-1], 500, 500, 2000, 10], n_clusters=10, init=init)\n",
    "\n",
    "dec.pretrain(x=train_x, y=train_y, optimizer=pretrain_optimizer,\n",
    "             epochs=pretrain_epochs, batch_size=batch_size,\n",
    "             save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 10)                100       \n",
      "=================================================================\n",
      "Total params: 1,665,110\n",
      "Trainable params: 1,665,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 200\n",
      "Save interval 146.484375\n",
      "Initializing cluster centers with k-means.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jbang36/miniconda2/envs/pp36/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: acc = 0.05088, nmi = 0.77332, ari = 0.72686  ; loss= 0\n",
      "saving model to: results/DEC_model_0.h5\n",
      "Iter 200: acc = 0.05050, nmi = 0.77894, ari = 0.73977  ; loss= 0\n",
      "Iter 400: acc = 0.05045, nmi = 0.79176, ari = 0.75790  ; loss= 0\n",
      "Iter 600: acc = 0.04940, nmi = 0.80387, ari = 0.77173  ; loss= 0\n",
      "Iter 800: acc = 0.04915, nmi = 0.81126, ari = 0.77959  ; loss= 0\n",
      "Iter 1000: acc = 0.04907, nmi = 0.81544, ari = 0.78393  ; loss= 0\n",
      "Iter 1200: acc = 0.04887, nmi = 0.81856, ari = 0.78698  ; loss= 0\n",
      "Iter 1400: acc = 0.04897, nmi = 0.82119, ari = 0.78957  ; loss= 0\n",
      "Iter 1600: acc = 0.04893, nmi = 0.82321, ari = 0.79143  ; loss= 0\n",
      "Iter 1800: acc = 0.04865, nmi = 0.82513, ari = 0.79332  ; loss= 0\n",
      "Iter 2000: acc = 0.04868, nmi = 0.82601, ari = 0.79421  ; loss= 0\n",
      "Iter 2200: acc = 0.04862, nmi = 0.82645, ari = 0.79450  ; loss= 0\n",
      "Iter 2400: acc = 0.04857, nmi = 0.82719, ari = 0.79518  ; loss= 0\n",
      "Iter 2600: acc = 0.04857, nmi = 0.82789, ari = 0.79591  ; loss= 0\n",
      "Iter 2800: acc = 0.04865, nmi = 0.82805, ari = 0.79581  ; loss= 0\n",
      "Iter 3000: acc = 0.04860, nmi = 0.82827, ari = 0.79602  ; loss= 0\n",
      "Iter 3200: acc = 0.04872, nmi = 0.82841, ari = 0.79629  ; loss= 0\n",
      "Iter 3400: acc = 0.04878, nmi = 0.82864, ari = 0.79620  ; loss= 0\n",
      "Iter 3600: acc = 0.04885, nmi = 0.82860, ari = 0.79612  ; loss= 0\n",
      "Iter 3800: acc = 0.04887, nmi = 0.82872, ari = 0.79633  ; loss= 0\n",
      "Iter 4000: acc = 0.04888, nmi = 0.82899, ari = 0.79633  ; loss= 0\n",
      "Iter 4200: acc = 0.04888, nmi = 0.82891, ari = 0.79625  ; loss= 0\n",
      "Iter 4400: acc = 0.04897, nmi = 0.82885, ari = 0.79626  ; loss= 0\n",
      "Iter 4600: acc = 0.04897, nmi = 0.82919, ari = 0.79641  ; loss= 0\n",
      "Iter 4800: acc = 0.04888, nmi = 0.82933, ari = 0.79656  ; loss= 0\n",
      "Iter 5000: acc = 0.04882, nmi = 0.82950, ari = 0.79674  ; loss= 0\n",
      "Iter 5200: acc = 0.04888, nmi = 0.82934, ari = 0.79652  ; loss= 0\n",
      "Iter 5400: acc = 0.04888, nmi = 0.82938, ari = 0.79660  ; loss= 0\n",
      "Iter 5600: acc = 0.04888, nmi = 0.82939, ari = 0.79665  ; loss= 0\n",
      "Iter 5800: acc = 0.04893, nmi = 0.82951, ari = 0.79673  ; loss= 0\n",
      "Iter 6000: acc = 0.04897, nmi = 0.82957, ari = 0.79675  ; loss= 0\n",
      "Iter 6200: acc = 0.04892, nmi = 0.82969, ari = 0.79689  ; loss= 0\n",
      "Iter 6400: acc = 0.04897, nmi = 0.82967, ari = 0.79673  ; loss= 0\n",
      "Iter 6600: acc = 0.04893, nmi = 0.82976, ari = 0.79684  ; loss= 0\n",
      "Iter 6800: acc = 0.04895, nmi = 0.82972, ari = 0.79680  ; loss= 0\n",
      "delta_label  8.333333333333333e-05 < tol  0.0001\n",
      "Reached tolerance threshold. Stopping training.\n",
      "saving model to: results/DEC_model_final.h5\n"
     ]
    }
   ],
   "source": [
    "y_pred = dec.fit(train_x, y=train_y, tol=tol, maxiter=maxiter, batch_size=batch_size,\n",
    "                 update_interval=update_interval, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = dec.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8351783283745353"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_score = normalized_mutual_info_score(test_y, pred_val)\n",
    "dec_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Need to plot the accuracies and save the results\n",
    "# kmeans_score = 0.49\n",
    "# auto_score = 0.8\n",
    "# dec_score = 0.835\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.bar([0,1,2], [kmeans_score, auto_score, dec_score])\n",
    "plt.xticks([0,1,2], ['kmeans', 'auto', 'DEC'], rotation=30)\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores for K-means, Autoencoder, DEC')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "savefig('scores_for_prototype.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pp36",
   "language": "python",
   "name": "pp36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
